{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        " - A parameter is a variable or a value that is passed to a function, procedure, or module to customize its behavior or output. It is a specific input that is used to influence the calculation or outcome of a model, algorithm, or system.\n",
        "\n",
        "2. What is correlation? What does negative correlation mean?\n",
        " - Correlation refers to the relationship between two or more variables, indicating how changes in one variable affect the other. It's a statistical measure that helps identify patterns, trends, or associations between variables.\n",
        " negative correlation happens when one variable increases, the other decreases, and vice versa.For example, a negative correlation between exercise frequency and body fat percentage would indicate that as exercise frequency increases, body fat percentage decreases. This means that engaging in regular exercise can lead to a reduction in body fat.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        " - Machine Learning (ML) is a subset of Artificial Intelligence (AI) that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "The main components of Machine Learning are:\n",
        "Data: The input provided to the algorithm to learn from.\n",
        "Algorithm: The mathematical model used to analyze the data and make predictions.\n",
        "Model: The output of the algorithm, which is a mathematical representation of the data.\n",
        "Training: The process of adjusting the algorithm to fit the data and improve its accuracy.\n",
        "Testing: The process of evaluating the model's performance on a separate dataset.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        " - A low loss value indicates a model is making accurate predictions, while a high loss value suggests poor performance, with the goal of training being to minimize the loss.\n",
        "  \n",
        "5. What are continuous and categorical variables?\n",
        " - continuous variables represent values that can take any value within a range (like height or weight), while categorical variables represent distinct groups or categories (like gender or color).\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning?  What are the common t echniques?\n",
        " - Handling categorical variables in Machine Learning is essential because most algorithms require numerical input. Categorical data represents types or labels and must be converted into a machine-readable format.\n",
        "\n",
        "Common techniques include Label Encoding, where each category is assigned a unique integer, and One-Hot Encoding, which creates binary columns for each category. Ordinal Encoding is used when categories have a natural order. For high-cardinality features, Target Encoding or Hash Encoding may be used. Choosing the right method depends on the algorithm and the nature of the data to ensure optimal model performance.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        " - raining a dataset means using a portion of the data to teach the model patterns and relationships. The model learns from this data to make predictions. Testing a dataset involves using a separate portion of the data, unseen by the model during training, to evaluate its performance. This helps assess how well the model generalizes to new, unseen data and ensures it’s not just memorizing the training data. A common split is 70% for training and 30% for testing, or similar ratios.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        " - sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare and transform data before training machine learning models. It includes functions for scaling, normalizing, encoding categorical variables, and handling missing values—helping improve model accuracy and performance.\n",
        "\n",
        "9. What is a Test set?\n",
        " - A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model. It helps check how well the model generalizes to new, unseen data.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        " - To split data for model fitting in Python, we commonly use the train_test_split() function from Scikit-learn. It divides the dataset into training and testing sets, often using an 80-20 or 70-30 ratio.\n",
        "\n",
        "When approaching a Machine Learning problem, start by understanding the data and the problem type (classification, regression, etc.). Next, clean and preprocess the data, split it, and choose a suitable algorithm. Then, train the model on the training set, evaluate it using the test set, and tune it for better performance. Finally, validate the results and deploy the model if it performs well.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        " - Exploratory Data Analysis (EDA) is essential before fitting a model because it helps us understand the structure, patterns, and relationships within the data. It reveals important insights like missing values, outliers, data distributions, and correlations between features. Performing EDA ensures better decision-making during preprocessing, feature selection, and model choice, ultimately leading to improved model accuracy and performance.\n",
        "\n",
        "12. What is correlation?\n",
        " - Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another. The value ranges from -1 to 1.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        " - Negative correlation means that as one variable increases, the other variable tends to decrease. In other words, they move in opposite directions.  \n",
        "\n",
        "For example, if the number of hours spent watching TV increases while exam scores decrease, these two variables have a negative correlation. The strength of the relationship is measured by a correlation coefficient close to -1.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        " - You can find the correlation between variables in Python using the `corr()` function provided by the pandas library. This method calculates the correlation matrix, showing how strongly each pair of variables is related. For more specific analysis between two variables, you can also use the `pearsonr()` function from the SciPy library, which returns the Pearson correlation coefficient and a p-value. These tools help understand the strength and direction of relationships in your dataset.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation\n",
        "with an example.\n",
        " -\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each\n",
        "with an example.\n",
        " - An optimizer in machine learning is an algorithm that adjusts model parameters like weights and biases to minimize the loss function and improve model accuracy during training.\n",
        "\n",
        "There are different types of optimizers, each with unique update rules. Gradient Descent is the most basic, updating weights in the direction of the steepest loss reduction. Stochastic Gradient Descent (SGD) updates using one sample at a time, making it faster but noisier. Adam combines momentum and adaptive learning rates for efficient training.\n",
        "EXAMPLE: optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        " - `sklearn.linear_model` is a module in Scikit-learn that provides linear models for regression and classification tasks. It includes popular algorithms like **LinearRegression**, **LogisticRegression**, **Ridge**, and **Lasso**. These models assume a linear relationship between input features and the target variable. For example, `LinearRegression()` is used for predicting continuous values, while `LogisticRegression()` is used for binary or multi-class classification problems.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        " - The `model.fit()` function is used to train a machine learning model on given data. It adjusts the model’s parameters based on the input features and target values.\n",
        "\n",
        "It typically requires at least two main arguments:  \n",
        "- the input features (independent variables)  \n",
        "- the target labels (dependent variable)  \n",
        "\n",
        "For example:  \n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        " - The `model.predict()` function is used to make predictions using a trained machine learning model. It takes new input data and returns the model’s predicted output.\n",
        "\n",
        "The main argument required is:  \n",
        "- the input features (same format as used in training)\n",
        "\n",
        "For example:  \n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        " - Continuous variables are numeric variables that can take any value within a range. They are measurable and often represent quantities like height, weight, temperature, or income.\n",
        "\n",
        "Categorical variables represent distinct groups or categories. They can be nominal (no order, e.g., color, gender) or **ordinal** (with order, e.g., low, medium, high). These variables are typically labels or names used to classify data.  \n",
        "\n",
        "Understanding the type of variable is important for choosing the right preprocessing and modeling techniques in machine learning.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        " - Feature scaling is a preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset. It ensures that no feature dominates others due to its scale.\n",
        "\n",
        "In Machine Learning, feature scaling helps improve model performance and training speed, especially for algorithms like KNN, SVM, and Gradient Descent-based models. Without scaling, features with larger values can bias the model.  \n",
        "\n",
        "Common methods include Min-Max Scaling (scales between 0 and 1) and Standardization (scales to have a mean of 0 and standard deviation of 1).\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        " - We perform scaling in Python using preprocessing tools from the Scikit-learn library. Two common methods are:\n",
        "\n",
        "1. StandardScaler – scales data to have mean = 0 and standard deviation = 1.  \n",
        "2. MinMaxScaler – scales data to a fixed range, usually [0, 1].\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()  # or MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        " - sklearn.preprocessing is a module in the Scikit-learn library that provides tools for preparing and transforming data before training machine learning models. It includes functions for scaling features, encoding categorical variables, normalizing data, and handling missing values. These preprocessing steps help improve model performance and ensure that data is in a suitable format for learning algorithms.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        " - In Python, we split data for model fitting using the train_test_split() function from the Scikit-learn library. It divides the dataset into training and testing sets.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "- X is the feature data  \n",
        "- y is the target/label  \n",
        "- test_size defines the proportion of data for testing (e.g., 0.2 = 20%)  \n",
        "- random_state ensures reproducibility  \n",
        "\n",
        "This helps evaluate the model’s performance on unseen data.\n",
        "\n",
        "25. Explain data encoding?\n",
        " - Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that machine learning algorithms can process it. Most models work only with numbers, so encoding is essential for handling features like colors, labels, or categories.\n",
        "\n",
        "Common encoding techniques include:\n",
        "\n",
        "- Label Encoding: Assigns a unique integer to each category.  \n",
        "- One-Hot Encoding: Creates binary columns for each category (0 or 1).  \n",
        "- Ordinal Encoding: Encodes categories with a meaningful order (e.g., low=1, medium=2, high=3).  \n",
        "\n",
        "Encoding helps the model understand and learn from categorical data effectively."
      ],
      "metadata": {
        "id": "7ijTQiXduzDx"
      }
    }
  ]
}